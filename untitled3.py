# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hi6BLFtyjb9N29Vl1kOv-hSjyHM2zT35
"""

!pip install torchaudio kaggle

!mkdir -p ~/.kaggle
!cp "/content/drive/MyDrive/ASVspoof2021/kagledataset/kaggle.json" ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets list

!kaggle datasets download -d pratikjodgudri/asvspoof2021-df-audio-dataset
!unzip -q asvspoof2021-df-audio-dataset.zip -d dataset

import torchaudio
import torchaudio.transforms as T
import torch
import os

# Define paths based on where the dataset was unzipped.
# For example, if the unzip command created a folder named "dataset" in the current directory:
RAW_DATASET_PATH = "dataset/DF_audio"          # This folder should contain subfolders "bonafide" and "spoofed"
PROCESSED_DATASET_PATH = "dataset/processed_DF_audio"

# Audio settings
TARGET_SAMPLE_RATE = 16000  # Standardized sample rate (16 kHz)
TARGET_LENGTH = 32000       # Fixed length in samples (2 seconds at 16 kHz)

# Create the output directory if it doesn't exist
os.makedirs(PROCESSED_DATASET_PATH, exist_ok=True)

def preprocess_audio(file_path, output_path):
    try:
        # Load the audio file
        waveform, sample_rate = torchaudio.load(file_path)

        # Resample if the sample rate is not the target sample rate
        if sample_rate != TARGET_SAMPLE_RATE:
            resample = T.Resample(orig_freq=sample_rate, new_freq=TARGET_SAMPLE_RATE)
            waveform = resample(waveform)

        # Pad if the audio is too short, or truncate if it's too long
        if waveform.shape[1] < TARGET_LENGTH:
            pad_length = TARGET_LENGTH - waveform.shape[1]
            waveform = torch.nn.functional.pad(waveform, (0, pad_length))  # Zero-padding
        else:
            waveform = waveform[:, :TARGET_LENGTH]  # Truncate to target length

        # Save the processed audio file
        torchaudio.save(output_path, waveform, TARGET_SAMPLE_RATE)
        return True
    except Exception as e:
        print(f"âŒ Error processing {file_path}: {e}")
        return False

# Process all files in "bonafide" and "spoofed" subdirectories
for subdir in ["bonafide", "spoofed"]:
    input_dir = os.path.join(RAW_DATASET_PATH, subdir)
    output_dir = os.path.join(PROCESSED_DATASET_PATH, subdir)
    os.makedirs(output_dir, exist_ok=True)

    # Get all FLAC files in the subdirectory and sort them
    files = sorted([f for f in os.listdir(input_dir) if f.endswith(".flac")])

    print(f"Processing {len(files)} files in '{subdir}'...")
    for idx, file_name in enumerate(files):
        input_path = os.path.join(input_dir, file_name)
        output_name = f"{idx:05d}.flac"  # Rename files sequentially (e.g., 00000.flac, 00001.flac, ...)
        output_path = os.path.join(output_dir, output_name)

        if preprocess_audio(input_path, output_path):
            print(f"âœ… Processed: {file_name} âž {output_name}")

print("\nðŸŽ‰ All audio files have been processed and saved in the 'processed_DF_audio' directory!")

# List processed files in the bonafide folder
processed_bonafide = os.listdir(os.path.join(PROCESSED_DATASET_PATH, "bonafide"))
print("Processed bonafide files:", processed_bonafide[:5])

# List processed files in the spoofed folder
processed_spoofed = os.listdir(os.path.join(PROCESSED_DATASET_PATH, "spoofed"))
print("Processed spoofed files:", processed_spoofed[:5])

# Install required packages (if not already installed)
!pip install torchaudio gradio

# Standard Python and PyTorch imports
import os
import torch
import torchaudio
import torchaudio.transforms as T
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader, random_split

class ASVspoofDataset(Dataset):
    def __init__(self, base_path, subdirs=["bonafide", "spoofed"]):
        """
        base_path: path to the processed dataset folder (should contain subfolders for each class)
        subdirs: list of subdirectory names; here, "bonafide" (label 0) and "spoofed" (label 1)
        """
        self.files = []
        self.labels = []
        for label, subdir in enumerate(subdirs):
            folder = os.path.join(base_path, subdir)
            # Get a sorted list of all FLAC files
            flac_files = sorted([os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(".flac")])
            self.files.extend(flac_files)
            self.labels.extend([label] * len(flac_files))
        # Shuffle the dataset (optional)
        perm = np.random.permutation(len(self.files))
        self.files = np.array(self.files)[perm].tolist()
        self.labels = np.array(self.labels)[perm].tolist()

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = self.files[idx]
        label = self.labels[idx]
        waveform, sr = torchaudio.load(file_path)
        # Ensure the audio is mono (average if multi-channel)
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        return waveform, label

# (Optional) Verify a few file names:
PROCESSED_DATASET_PATH = "dataset/processed_DF_audio"
print("Bonafide files:", os.listdir(os.path.join(PROCESSED_DATASET_PATH, "bonafide"))[:5])
print("Spoofed files:", os.listdir(os.path.join(PROCESSED_DATASET_PATH, "spoofed"))[:5])

class SmallAASIST(nn.Module):
    def __init__(self):
        super(SmallAASIST, self).__init__()
        # Input: (batch, 1, 32000)
        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=2, padding=1)  # Output: (batch, 16, 16000)
        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=2, padding=1)  # Output: (batch, 32, 8000)
        self.pool = nn.MaxPool1d(kernel_size=2)                              # Output after pool: (batch, 32, 4000) after conv1+pool; then (batch, 32, 2000) after conv2+pool.
        # After conv1 -> pool: 32000/2 -> pool (kernel=2): 32000/4 = 8000;
        # then conv2 (stride 2): 8000/2 = 4000; then pool: 4000/2 = 2000.
        self.flat_size = 32 * 2000  # 64000 features
        self.fc1 = nn.Linear(self.flat_size, 64)
        self.fc2 = nn.Linear(64, 2)  # 2 output classes: bonafide (0) and spoofed (1)
        self.dropout = nn.Dropout(0.3)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.pool(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = self.pool(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Test the model with a dummy input
dummy_input = torch.randn(1, 1, 32000)  # (batch_size, channels, samples)
model = SmallAASIST()
print("Model output shape:", model(dummy_input).shape)

# Create the full dataset
full_dataset = ASVspoofDataset(PROCESSED_DATASET_PATH)

# Split: 80% training, 20% validation
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

# Create DataLoaders
batch_size = 4  # Adjust based on available memory
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

print("Number of training samples:", len(train_dataset))
print("Number of validation samples:", len(val_dataset))

def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-4):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Using device:", device)
    model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    best_val_acc = 0.0
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        for i, (inputs, targets) in enumerate(train_loader):
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            correct += (preds == targets).sum().item()
            total += targets.size(0)
        train_loss = running_loss / len(train_loader)
        train_acc = correct / total * 100
        print(f"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")

        # Evaluate on validation set
        model.eval()
        val_correct = 0
        val_total = 0
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                val_correct += (preds == targets).sum().item()
                val_total += targets.size(0)
        val_acc = val_correct / val_total * 100
        print(f"Validation Accuracy: {val_acc:.2f}%")

        # Save the best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "best_model.pth")
            print("Best model saved with accuracy: {:.2f}%".format(val_acc))

    print("Training complete!")

model = SmallAASIST()
train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-4)

# Load the best model (using CPU here; you can load on GPU if needed)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.load_state_dict(torch.load("best_model.pth", map_location=device))
model.to(device)
model.eval()

def predict_audio(file_path):
    """
    Loads an audio file, ensures it is mono and 2 seconds long (32000 samples),
    then returns the model prediction.
    """
    waveform, sr = torchaudio.load(file_path)
    # If multi-channel, convert to mono
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)
    # Pad or truncate to 32000 samples
    if waveform.shape[1] < 32000:
        pad_length = 32000 - waveform.shape[1]
        waveform = torch.nn.functional.pad(waveform, (0, pad_length))
    else:
        waveform = waveform[:, :32000]
    waveform = waveform.unsqueeze(0).to(device)  # Add batch dimension
    with torch.no_grad():
        output = model(waveform)
        _, pred = torch.max(output, 1)
    # Return the prediction: 0 -> Bonafide (Real), 1 -> Spoofed (Fake)
    return "Bonafide (Real)" if pred.item() == 0 else "Spoofed (Fake)"

# Test the function on a sample file from the bonafide folder:
sample_file = os.path.join(PROCESSED_DATASET_PATH, "bonafide", os.listdir(os.path.join(PROCESSED_DATASET_PATH, "bonafide"))[0])
print("Prediction for sample file:", predict_audio(sample_file))

import gradio as gr

def infer_audio(audio_file):
    # audio_file is the temporary path provided by Gradio
    return predict_audio(audio_file)

iface = gr.Interface(
    fn=infer_audio,
    inputs=gr.Audio(source="upload", type="filepath"),
    outputs="text",
    title="Audio Deepfake Detection",
    description="Upload an audio file to determine if it is bonafide (real) or spoofed (fake)."
)

iface.launch()